import requests
import pandas as pd
import numpy as np
from ta import add_all_ta_features # Technical Analysis library
from sklearn.model_selection import train_test_split # Using train_test_split for simplicity, sequential split is often better for time series
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import joblib
import time

# --- Configuration ---
API_URL = "https://api.coingecko.com/api/v3/coins/sui/market_chart?vs_currency=usd&days=30"
MODEL_FILENAME = "model.pkl"
SCALER_FILENAME = "scaler.pkl"
FEATURES_FILENAME = "features.pkl"
# Simple Strategy Parameters (adjust as needed)
SMA_SHORT = 5   # Short term moving average window
SMA_LONG = 15   # Long term moving average window
RSI_PERIOD = 14
RSI_OVERBOUGHT = 70
RSI_OVERSOLD = 30

# --- Functions ---

def fetch_data(url):
    """Fetches data from the CoinGecko API."""
    print(f"Fetching data from {url}...")
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes
        data = response.json()
        print("Data fetched successfully.")
        return data
    except requests.exceptions.RequestException as e:
        print(f"Error fetching data: {e}")
        return None

def preprocess_data(raw_data):
    """Converts raw API data into a Pandas DataFrame."""
    if not raw_data or 'prices' not in raw_data:
        print("Invalid raw data received.")
        return None

    print("Preprocessing data...")
    prices = raw_data['prices']
    df = pd.DataFrame(prices, columns=['timestamp_ms', 'price'])

    # Convert timestamp to datetime and set as index
    df['timestamp'] = pd.to_datetime(df['timestamp_ms'], unit='ms')
    df = df.set_index('timestamp')
    df = df[['price']] # Keep only the price column for now

    # Optional: Add market caps and volumes if needed for features
    if 'market_caps' in raw_data and len(raw_data['market_caps']) == len(df):
         mcaps = pd.DataFrame(raw_data['market_caps'], columns=['timestamp_ms', 'market_cap'])
         mcaps['timestamp'] = pd.to_datetime(mcaps['timestamp_ms'], unit='ms')
         mcaps = mcaps.set_index('timestamp')[['market_cap']]
         df = df.join(mcaps)

    if 'total_volumes' in raw_data and len(raw_data['total_volumes']) == len(df):
         vols = pd.DataFrame(raw_data['total_volumes'], columns=['timestamp_ms', 'volume'])
         vols['timestamp'] = pd.to_datetime(vols['timestamp_ms'], unit='ms')
         vols = vols.set_index('timestamp')[['volume']]
         df = df.join(vols)

    # Handle potential duplicate indices (can happen with API)
    df = df[~df.index.duplicated(keep='first')]

    # Ensure data is sorted by time
    df = df.sort_index()

    print(f"Preprocessing complete. DataFrame shape: {df.shape}")
    # print("Data head:\n", df.head()) # Uncomment to inspect
    return df

def feature_engineering(df):
    """Adds technical indicators as features."""
    print("Engineering features...")
    if df is None or df.empty:
        print("Cannot engineer features on empty DataFrame.")
        return None, []

    # Ensure required columns exist for ta library
    df['Open'] = df['price'].shift(1) # Approximate Open
    df['High'] = df[['price', 'Open']].max(axis=1) # Approximate High
    df['Low'] = df[['price', 'Open']].min(axis=1) # Approximate Low
    df['Close'] = df['price'] # Close is the current price
    df['Volume'] = df['volume'] if 'volume' in df.columns else df['price'].diff().abs() * 1000 # Approximate volume if real volume is missing

    # Drop rows with NaNs created by shift before calculating indicators
    df.dropna(subset=['Open'], inplace=True)
    if df.empty:
         print("DataFrame empty after dropping initial NaN for 'Open'. Not enough data.")
         return None, []

    # Add TA features using the 'ta' library
    # This adds many indicators, we might select specific ones later
    try:
        add_all_ta_features(
            df, open="Open", high="High", low="Low", close="Close", volume="Volume", fillna=True # fillna=True handles NaNs generated by TA calculations
        )
    except Exception as e:
        print(f"Error adding TA features: {e}")
        # Fallback: Calculate basic SMAs and RSI manually if ta fails or for simplicity
        df[f'SMA_{SMA_SHORT}'] = df['Close'].rolling(window=SMA_SHORT).mean()
        df[f'SMA_{SMA_LONG}'] = df['Close'].rolling(window=SMA_LONG).mean()
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=RSI_PERIOD).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=RSI_PERIOD).mean()
        rs = gain / loss
        df['RSI'] = 100 - (100 / (1 + rs))
        df.fillna(method='bfill', inplace=True) # Backfill remaining NaNs

    # Add simple price change features
    df['price_change_1'] = df['Close'].pct_change(periods=1)
    df['price_change_3'] = df['Close'].pct_change(periods=3)

    # Drop helper columns and NaNs
    df.drop(['Open', 'High', 'Low', 'Close', 'Volume', 'price'], axis=1, errors='ignore', inplace=True) # Drop original price if Close exists
    df.dropna(inplace=True) # Drop rows with NaNs from rolling calculations/pct_change

    # Define feature list dynamically (all columns except potential target)
    feature_cols = [col for col in df.columns if col != 'target'] # Exclude target if already present

    print(f"Feature engineering complete. Shape after TA and dropna: {df.shape}")
    if df.empty:
        print("DataFrame is empty after feature engineering and NaN removal.")
        return None, []

    # print("Features head:\n", df.head()) # Uncomment to inspect
    # print("Feature columns:", feature_cols)
    return df, feature_cols

def define_target(df):
    """Defines the target variable (Buy=1, Sell=-1, Hold=0) based on a simple strategy."""
    print("Defining target variable...")
    if df is None or df.empty:
        print("Cannot define target on empty DataFrame.")
        return None

    # --- Simple Strategy Example: SMA Crossover + RSI ---
    # This is just one example, many strategies are possible!

    conditions = [
        # Buy signal: Price above short SMA, short SMA above long SMA, RSI not overbought
        (df[f'SMA_{SMA_SHORT}'] > df[f'SMA_{SMA_LONG}']) & (df['trend_rsi'] < RSI_OVERBOUGHT),
        # Sell signal: Price below short SMA, short SMA below long SMA, RSI not oversold
        (df[f'SMA_{SMA_SHORT}'] < df[f'SMA_{SMA_LONG}']) & (df['trend_rsi'] > RSI_OVERSOLD),
    ]
    choices = [1, -1]  # 1 for Buy, -1 for Sell

    df['target'] = np.select(conditions, choices, default=0) # 0 for Hold

    # Optional: Shift target to predict the *next* period's action based on *current* features
    # df['target'] = df['target'].shift(-1)
    # df.dropna(subset=['target'], inplace=True) # Remove last row where target is NaN

    print("Target definition complete.")
    print("Target distribution:\n", df['target'].value_counts(normalize=True))
    return df


# --- Main Execution ---
if __name__ == "__main__":
    # 1. Fetch Data
    raw_data = fetch_data(API_URL)
    if raw_data is None:
        exit()

    # 2. Preprocess Data
    df = preprocess_data(raw_data)
    if df is None or df.empty:
        print("Exiting due to preprocessing error or insufficient data.")
        exit()
    # Make a copy before feature engineering if needed for other analysis
    df_processed = df.copy()

    # 3. Feature Engineering
    df_features, feature_names = feature_engineering(df_processed)
    if df_features is None or df_features.empty or not feature_names:
        print("Exiting due to feature engineering error or no features.")
        exit()

    # Ensure required columns for the strategy exist after TA feature addition
    required_cols = [f'SMA_{SMA_SHORT}', f'SMA_{SMA_LONG}', 'trend_rsi'] # trend_rsi is generated by ta library
    if not all(col in df_features.columns for col in required_cols):
         # Fallback if 'trend_rsi' isn't generated (maybe library issue or different version)
         if 'RSI' in df_features.columns and 'trend_rsi' not in required_cols:
             required_cols = [f'SMA_{SMA_SHORT}', f'SMA_{SMA_LONG}', 'RSI']
             # Rename RSI to trend_rsi internally for consistency if using fallback
             if 'trend_rsi' not in df_features.columns: df_features.rename(columns={'RSI': 'trend_rsi'}, inplace=True)

         if not all(col in df_features.columns for col in required_cols):
            print(f"Error: Missing one or more required columns for target definition: {required_cols}")
            print("Available columns:", df_features.columns.tolist())
            exit()


    # 4. Define Target Variable
    df_final = define_target(df_features.copy()) # Use copy to avoid modifying df_features directly
    if df_final is None or 'target' not in df_final.columns:
         print("Exiting due to target definition error.")
         exit()

    # Ensure feature_names doesn't include 'target' before splitting
    if 'target' in feature_names:
        feature_names.remove('target') # Should have been excluded already, but double-check

    # 5. Prepare Data for Model
    X = df_final[feature_names]
    y = df_final['target']

    if X.empty or y.empty:
        print("Exiting: Features (X) or target (y) is empty after processing.")
        exit()

    # --- Data Scaling ---
    print("Scaling features...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # --- Train/Test Split (Simple Random Split) ---
    # Note: For time series, a sequential split is generally preferred to avoid data leakage.
    # X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)
    # --- Sequential Split ---
    split_index = int(len(X_scaled) * 0.8)
    X_train, X_test = X_scaled[:split_index], X_scaled[split_index:]
    y_train, y_test = y[:split_index], y[split_index:]

    if len(X_train) == 0 or len(X_test) == 0:
        print(f"Exiting: Not enough data for train/test split. Train size: {len(X_train)}, Test size: {len(X_test)}")
        exit()

    print(f"Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}")

    # 6. Train Model
    print("Training RandomForestClassifier model...")
    # Use class_weight='balanced' if target classes are imbalanced
    model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)
    start_time = time.time()
    model.fit(X_train, y_train)
    end_time = time.time()
    print(f"Model training completed in {end_time - start_time:.2f} seconds.")

    # 7. Evaluate Model
    print("Evaluating model...")
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Model Accuracy on Test Set: {accuracy:.4f}")
    print("Classification Report:\n", classification_report(y_test, y_pred, target_names=['Sell (-1)', 'Hold (0)', 'Buy (1)'], zero_division=0))

    # Optional: Feature Importances
    # importances = pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False)
    # print("\nFeature Importances:\n", importances.head(10))


    # 8. Save Model, Scaler, and Feature List
    print(f"Saving model to {MODEL_FILENAME}")
    joblib.dump(model, MODEL_FILENAME)
    print(f"Saving scaler to {SCALER_FILENAME}")
    joblib.dump(scaler, SCALER_FILENAME)
    print(f"Saving feature list to {FEATURES_FILENAME}")
    joblib.dump(feature_names, FEATURES_FILENAME) # Save the list of feature names used for training

    print("\nTraining script finished.")